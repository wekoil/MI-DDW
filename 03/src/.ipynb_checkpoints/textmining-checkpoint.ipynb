{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importy\n",
    "import nltk\n",
    "import wikipedia\n",
    "from collections import Counter\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Top 20\n",
      "[((',', ','), 753), (('.', '.'), 731), (('the', 'DT'), 684), (('and', 'CC'), 488), (('to', 'TO'), 435), (('of', 'IN'), 377), (('in', 'IN'), 254), (('a', 'DT'), 218), (('is', 'VBZ'), 203), (('this', 'DT'), 143), (('will', 'MD'), 138), (('we', 'PRP'), 137), (('for', 'IN'), 123), (('We', 'PRP'), 111), (('I', 'PRP'), 107), (('our', 'PRP$'), 94), (('be', 'VB'), 92), (('that', 'IN'), 91), (('are', 'VBP'), 90), (('have', 'VBP'), 86)]\n",
      "\n",
      "POS Top 20 without punctuation\n",
      "[(('the', 'DT'), 684), (('and', 'CC'), 488), (('to', 'TO'), 435), (('of', 'IN'), 377), (('in', 'IN'), 254), (('a', 'DT'), 218), (('is', 'VBZ'), 203), (('this', 'DT'), 143), (('will', 'MD'), 138), (('we', 'PRP'), 137), (('for', 'IN'), 123), (('We', 'PRP'), 111), (('I', 'PRP'), 107), (('our', 'PRP$'), 94), (('be', 'VB'), 92), (('that', 'IN'), 91), (('are', 'VBP'), 90), (('have', 'VBP'), 86), (('Russia', 'NNP'), 79), (('must', 'MD'), 78)]\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos = nltk.pos_tag(tokens)\n",
    "\n",
    "count = Counter(pos)\n",
    "sort_pos = sorted(count.items(), key=lambda count:count[1], reverse=True)\n",
    "print('POS Top 20')\n",
    "print(sort_pos[:20])\n",
    "print()\n",
    "\n",
    "nopunc = [token for token in tokens if token not in punctuation]\n",
    "tagged = nltk.pos_tag(nopunc)\n",
    "\n",
    "count = Counter(tagged)\n",
    "sort_tagged = sorted(count.items(), key=lambda count:count[1], reverse=True)\n",
    "print('POS Top 20 without punctuation')\n",
    "print(sort_tagged[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with entity classification (using nltk.ne_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Top 20\n",
      "[('Yekaterinburg', 'GSP'), ('Syria', 'GSP'), ('Federation Council', 'ORGANIZATION'), ('State Duma', 'ORGANIZATION'), ('Address', 'ORGANIZATION'), ('GDP', 'ORGANIZATION'), ('Particular', 'ORGANIZATION'), ('Technical', 'ORGANIZATION'), ('Crimean Bridge', 'ORGANIZATION'), ('Azov', 'ORGANIZATION'), ('Arctic', 'ORGANIZATION'), ('Spatial Development Strategy', 'ORGANIZATION'), ('Extreme North', 'ORGANIZATION'), ('Popular', 'ORGANIZATION'), ('Medical', 'ORGANIZATION'), ('Volunteers', 'ORGANIZATION'), ('NPOs', 'ORGANIZATION'), ('Sciences', 'ORGANIZATION'), ('Council', 'ORGANIZATION'), ('Science', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "ne_chunked = nltk.ne_chunk(tagged)\n",
    "ner = {}\n",
    "for entity in ne_chunked:\n",
    "    if isinstance(entity, nltk.tree.Tree):\n",
    "        text = \" \".join([word for word, tag in entity.leaves()])\n",
    "        ent = entity.label()\n",
    "        ner[text] = ent\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "sort_ner = sorted(ner.items(), key=lambda entity: entity[1][1], reverse=True)\n",
    "print('NER Top 20')\n",
    "print(sort_ner[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with custom patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom NER Top 20\n",
      "[('Citizens', 'NP'), ('members', 'NP'), ('special landmark', 'NP'), ('event', 'NP'), ('the times', 'NP'), ('the choices', 'NP'), ('every step', 'NP'), ('the future', 'NP'), ('country', 'NP'), ('decades', 'NP'), ('points', 'NP'), ('time', 'NP'), ('ability', 'NP'), ('new territories', 'NP'), ('build cities', 'NP'), ('conquer', 'NP'), ('space', 'NP'), ('major discoveries', 'NP'), ('This unwavering forward-looking drive', 'NP'), ('traditions', 'NP')]\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "text_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN|NNS>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(text_pos)\n",
    "custom_ner = {}\n",
    "for entity in result:\n",
    "    if isinstance(entity, nltk.tree.Tree):\n",
    "        text = \" \".join([word for word, tag in entity.leaves()])\n",
    "        ent = entity.label()\n",
    "        custom_ner[text] = ent\n",
    "    else:\n",
    "        continue\n",
    "sort_custom_ner = sorted(custom_ner.items(), key=lambda entity: entity[1][1], reverse=True)\n",
    "print('Custom NER Top 20')\n",
    "print(sort_custom_ner[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom entity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wiki(name):\n",
    "    try:\n",
    "        page = wikipedia.page(name)\n",
    "        summary = page.summary\n",
    "    except:\n",
    "        return \"\"\n",
    "    return nltk.sent_tokenize(summary)[0]\n",
    "\n",
    "def wikidescription(name):\n",
    "    sent = wiki(name)\n",
    "    if sent == \"\":\n",
    "        return \"a Thing\"\n",
    "    \n",
    "    text_pos = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    \n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN|NNS>}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(text_pos)\n",
    "    data = {}\n",
    "    for entity in result:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    str = \"\"\n",
    "    for data in data:\n",
    "        str+=data\n",
    "        if (str[-1] != ' '):\n",
    "            str += ' '\n",
    "    return str\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikipedia-based classification using nltk entities as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yekaterinburg  -  Екатеринбу́рг [ jɪkətʲɪrʲɪnˈburk the fourth-largest city the administrative centre east the middle the Eurasian continent the Asian side the boundary \n",
      "Syria  -  الجمهورية السورية‎ al-Jumhūrīyah al-ʻArabīyah as-Sūrīyah a country the southwest the west the north the east the south \n",
      "Federation Council  -  Сове́т Федера́ции common abbreviation Совфед the upper house the parliament \n",
      "State Duma  -  Госуда́рственная ду́ма tr \n",
      "Address  -  An address a collection information format the location a building apartment other structure a plot land political boundaries street names references other identifiers house numbers \n",
      "GDP  -  domestic product a monetary measure the market value the final goods services a period time capita reflect differences the cost living the inflation rates the countries a basis power parity differences standards nations \n",
      "Particular  -  metaphysics particulars concrete spatiotemporal entities entities properties numbers \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\majkl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\users\\majkl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical  -  a Thing\n",
      "Crimean Bridge  -  Крымский мост tr \n",
      "Azov  -  Азов the stress the second syllable a town kilometers mi name the town \n",
      "Arctic  -  a polar region part \n",
      "Spatial Development Strategy  -  a document \n",
      "Extreme North  -  Крайний Север a large part enormous mineral natural resources \n",
      "Popular  -  a Thing\n",
      "Medical  -  the science practice the diagnosis prognosis treatment prevention disease \n",
      "Volunteers  -  a Thing\n",
      "NPOs  -  a Thing\n",
      "Sciences  -  Science word scientia knowledge a systematic enterprise the form testable explanations predictions roots science \n",
      "Council  -  A council a group people decisions \n",
      "Science  -  Science word scientia knowledge a systematic enterprise the form testable explanations predictions roots science \n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in sort_ner:\n",
    "    print(i[0], ' - ', wikidescription(i[0]))\n",
    "    c+= 1\n",
    "    if c == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikipedia-based classification using custom patterns as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizens  -  Citizenship the status a person the custom law a legal member a sovereign state belonging a nation \n",
      "members  -  a Thing\n",
      "special landmark  -  a private college disabilities attention disorders autism \n",
      "event  -  a Thing\n",
      "the times  -  a British daily national newspaper \n",
      "the choices  -  decision making \n",
      "every step  -  Every Little Step American singer Edmonds \n",
      "the future  -  The future the time \n",
      "country  -  A country a region a distinct entity political geography \n",
      "decades  -  A decade a period years \n",
      "points  -  a Thing\n",
      "time  -  the indefinite continued progress existence events irreversible succession the past the future \n",
      "ability  -  a Thing\n",
      "new territories  -  新界 Cantonese Yale Sān'gaai main regions \n",
      "build cities  -  SimCity a city-building simulation mobile game \n",
      "conquer  -  a Thing\n",
      "space  -  Space the boundless three-dimensional extent objects events relative position direction \n",
      "major discoveries  -  The timeline the date publication possible major scientific theories discoveries the discoverer \n",
      "This unwavering forward-looking drive  -  The Indian general election members parliament parliamentary constituencies \n",
      "traditions  -  A tradition a belief behavior a group society symbolic meaning special significance origins the past \n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in sort_custom_ner:\n",
    "    print(i[0], ' - ', wikidescription(i[0]))\n",
    "    c+= 1\n",
    "    if c == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
