{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importy\n",
    "import nltk\n",
    "import wikipedia\n",
    "from collections import Counter\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Top 20\n",
      "[((',', ','), 753), (('.', '.'), 731), (('the', 'DT'), 684), (('and', 'CC'), 488), (('to', 'TO'), 435), (('of', 'IN'), 377), (('in', 'IN'), 254), (('a', 'DT'), 218), (('is', 'VBZ'), 203), (('this', 'DT'), 143), (('will', 'MD'), 138), (('we', 'PRP'), 137), (('for', 'IN'), 123), (('We', 'PRP'), 111), (('I', 'PRP'), 107), (('our', 'PRP$'), 94), (('be', 'VB'), 92), (('that', 'IN'), 91), (('are', 'VBP'), 90), (('have', 'VBP'), 86)]\n",
      "\n",
      "POS Top 20 without punctuation\n",
      "[(('the', 'DT'), 684), (('and', 'CC'), 488), (('to', 'TO'), 435), (('of', 'IN'), 377), (('in', 'IN'), 254), (('a', 'DT'), 218), (('is', 'VBZ'), 203), (('this', 'DT'), 143), (('will', 'MD'), 138), (('we', 'PRP'), 137), (('for', 'IN'), 123), (('We', 'PRP'), 111), (('I', 'PRP'), 107), (('our', 'PRP$'), 94), (('be', 'VB'), 92), (('that', 'IN'), 91), (('are', 'VBP'), 90), (('have', 'VBP'), 86), (('Russia', 'NNP'), 79), (('must', 'MD'), 78)]\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "pos = nltk.pos_tag(tokens)\n",
    "\n",
    "count = Counter(pos)\n",
    "sort_pos = sorted(count.items(), key=lambda count:count[1], reverse=True)\n",
    "print('POS Top 20')\n",
    "print(sort_pos[:20])\n",
    "print()\n",
    "\n",
    "nopunc = [token for token in tokens if token not in punctuation]\n",
    "tagged = nltk.pos_tag(nopunc)\n",
    "\n",
    "count = Counter(tagged)\n",
    "sort_tagged = sorted(count.items(), key=lambda count:count[1], reverse=True)\n",
    "print('POS Top 20 without punctuation')\n",
    "print(sort_tagged[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with entity classification (using nltk.ne_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Top 20\n",
      "[('Yekaterinburg', 'GSP'), ('Syria', 'GSP'), ('Federation Council', 'ORGANIZATION'), ('State Duma', 'ORGANIZATION'), ('Address', 'ORGANIZATION'), ('GDP', 'ORGANIZATION'), ('Particular', 'ORGANIZATION'), ('Technical', 'ORGANIZATION'), ('Crimean Bridge', 'ORGANIZATION'), ('Azov', 'ORGANIZATION'), ('Arctic', 'ORGANIZATION'), ('Spatial Development Strategy', 'ORGANIZATION'), ('Extreme North', 'ORGANIZATION'), ('Popular', 'ORGANIZATION'), ('Medical', 'ORGANIZATION'), ('Volunteers', 'ORGANIZATION'), ('NPOs', 'ORGANIZATION'), ('Sciences', 'ORGANIZATION'), ('Council', 'ORGANIZATION'), ('Science', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "ne_chunked = nltk.ne_chunk(tagged)\n",
    "ner = {}\n",
    "for entity in ne_chunked:\n",
    "    if isinstance(entity, nltk.tree.Tree):\n",
    "        text = \" \".join([word for word, tag in entity.leaves()])\n",
    "        ent = entity.label()\n",
    "        ner[text] = ent\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "sort_ner = sorted(ner.items(), key=lambda entity: entity[1][1], reverse=True)\n",
    "print('NER Top 20')\n",
    "print(sort_ner[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER with custom patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom NER Top 20\n",
      "[('Citizens', 'NP'), ('members', 'NP'), ('special landmark', 'NP'), ('event', 'NP'), ('the times', 'NP'), ('the choices', 'NP'), ('every step', 'NP'), ('the future', 'NP'), ('country', 'NP'), ('decades', 'NP'), ('points', 'NP'), ('time', 'NP'), ('ability', 'NP'), ('new territories', 'NP'), ('build cities', 'NP'), ('conquer', 'NP'), ('space', 'NP'), ('major discoveries', 'NP'), ('This unwavering forward-looking drive', 'NP'), ('traditions', 'NP')]\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "text_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN|NNS>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(text_pos)\n",
    "custom_ner = {}\n",
    "for entity in result:\n",
    "    if isinstance(entity, nltk.tree.Tree):\n",
    "        text = \" \".join([word for word, tag in entity.leaves()])\n",
    "        ent = entity.label()\n",
    "        custom_ner[text] = ent\n",
    "    else:\n",
    "        continue\n",
    "sort_custom_ner = sorted(custom_ner.items(), key=lambda entity: entity[1][1], reverse=True)\n",
    "print('Custom NER Top 20')\n",
    "print(sort_custom_ner[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Citizens of Russia', 'Federation Council', 'State Duma', 'Today', 'Address', 'Russia', 'Today', 'Russia', 'Therefore', 'President', 'Colleagues', 'Poverty', 'Today', 'Government', 'People', 'Russia', 'GDP', 'Russia', 'Russia', 'Life expectancy levels', 'Today', 'Russia', 'Japan', 'France', 'Germany', 'Colleagues', 'Cities like Kazan', 'Vladivostok', 'Sochi', 'Change', 'Initiatives', 'Address', 'People', 'Today', 'Russia', 'Russiaís', 'Colleagues', 'Russia', 'Next', 'December', 'Technical Inventory Bureau', 'Colleagues', 'Russiaís', 'Crimean Bridge', 'Black Sea region', 'Europe ñ Asia-Pacific corridor', 'Kazakhstani partners', 'Baikal-Amur Mainline', 'Railway', 'Russia', 'Asia', 'Soviet Union', 'Soviet Union', 'Azov', 'Black Sea', 'Northern Sea Route', 'Russian Arctic', 'Far East', 'Northern Sea Route', 'Arctic', 'Russiaís interests', 'Russia', 'Spatial Development Strategy', 'Government', 'Russia', 'Likewise', 'Extreme North', 'Siberia', 'Russian Far East', 'Colleagues', 'A', 'GDP', 'May executive orders', 'Wages', 'Popular Front', 'Disease prevention', 'Modern diagnostics', 'Colleagues', 'Colleagues', 'Medical assistance', 'A', 'Krasnoyarsk', 'Cherepovets', 'Nizhny Tagil', 'Chelyabinsk', 'Novokuznetsk', 'Lake Teletskoye', 'Volga Basin', 'Colleagues', 'Russia', 'Year of Volunteers', 'Today', 'NPOs contribute', 'Vladivostok', 'Colleagues', 'Russia', 'Schools', 'International experts', 'Russia', 'School administrators', 'Quantorium', 'Russia', 'Ticket', 'Future', 'Colleagues', 'Big Data processing technology', 'Internet of Things technologies', 'Arctic', 'Academy of Sciences', 'Russiaís', 'Building', 'Projects', 'Gatchina', 'Dubna', 'Council for Science', 'Education', 'Novosibirsk Akademgorodok', 'Moscow Region', 'Russia', 'Russian research teams', 'Russia', 'Large research', 'Kazan', 'Samara', 'Tomsk', 'Novosibirsk', 'Yekaterinburg', 'Tyumen', 'Vladivostok', 'Kaliningrad', 'Russia', 'International mathematics centres', 'Kazan', 'Novosibirsk', 'St Petersburg', 'Moscow', 'Sochi', 'Young Russians', 'Olympiad', 'WorldSkills competition', 'Russia', 'Colleagues', 'Question', 'First', 'Government', 'Government', 'Inflation', 'Anti-Monopoly Service', 'Russia', 'Business', 'Bank of Russia', 'Government', 'First', 'GDP', 'Government in conjunction', 'Bank of Russia', 'Investment', 'GDP', 'Exports of services', 'Nevertheless', 'Note', 'Soviet Union', 'USSR', 'Russia', 'Urals', 'Siberia', 'Added value', 'Colleagues', 'Code', 'Working Group on Monitoring', 'Analysing Law Enforcement Practice in Entrepreneurial Activity', 'Supreme Court', 'Prosecutorís Office', 'Colleagues', 'Internet for hours', 'Government officials', 'Leaders of Russia competition', 'Fatherland', 'Russia', 'Colleagues', 'Russian Armed Forces', 'Army', 'Navy', 'Armed Forces', 'Borei', 'Twelve missile regiments', 'Yars', 'Army', 'Aerospace Forces', 'Navy', 'Russia', 'Russiaís perimeter', 'USSR', 'National Defence Control Centre', 'Armed Forces', 'United States of America', 'Missile Treaty', 'US', 'Back', 'US', 'Missile Treaty', 'Russia', 'Soviet-US ABM Treaty', 'Russia', 'US', 'Grand Forks', 'ICBM base', 'Strategic Arms Reduction Treaty', 'ABM Treaty', 'Americans', 'US', 'Americans', 'US', 'BMD system against Russia', 'US', 'US', 'USSR', 'Russia', 'Soviet Union', 'Russia', 'GDP', 'Soviet Armed Forces', 'Armed Forces', 'A', 'Caucasus', 'US inspectors', 'USSR', 'Russia', 'IMF', 'World Bank', 'Armed Forces', 'Russiaís opinion', 'United States', 'Missile Treaty', 'Russia', 'US', 'New START treaty', 'US', 'Alaska', 'California', 'Europe', 'Romania', 'Japan', 'South Korea', 'US', 'Russiaís borders', 'How', 'US withdrawal', 'ABM Treaty', 'United States', 'Russia', 'Defence Ministry', 'Sarmat', 'Sarmat', 'Voevoda system', 'USSR', 'Sarmat missile', 'Voevodaís', 'Sarmat', 'Could', 'Video', 'Voevoda', 'North', 'South poles', 'Sarmat', 'Russia', 'Tomahawk missile', 'Russia', 'Central training ground', 'Video', 'December', 'Please', 'Video', 'Defence Ministry', 'Countries', 'Mach numbers in honour', 'Mach', 'Mach', 'Mach', 'Mach', 'Friends', 'Russia', 'December', 'Military District', 'Dagger', 'Video', 'Video', 'NATO members', 'US', 'Russia', 'Russian Armed Forces', 'Strategic Missile Forces', 'Russia', 'Video', 'Avangard', 'Russia ’', 'Video', 'Soviet Union', 'Thousands', 'President', 'Russia', 'Defence Ministry', 'General Staff', 'Russia', 'Russia', 'Sooner', 'Russian pilot Major Roman Filipov', 'NATO infrastructure closer', 'Russia', 'Russia', 'Russia', 'Russia', 'Earth', 'Russia reserves', 'Retaliation', 'Russia', 'UN', 'People ’', 'Republic of China', 'Russia', 'India', 'Russia', 'CSTO', 'Shanghai Cooperation Organisation', 'BRICS', 'UN', 'G20', 'APEC', 'United States', 'European Union', 'Russia', 'Economic Union seek', 'EAEU ’', 'Colleagues', 'Russia', 'Challenges', 'Russia', 'Thank']\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "with open('text.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "entity = []\n",
    "custom_ner = []\n",
    "for tagged_entry in tagged:\n",
    "    if(tagged_entry[1].startswith(\"NN\") or (entity and tagged_entry[1].startswith(\"IN\"))):\n",
    "        entity.append(tagged_entry)\n",
    "    else:\n",
    "        if(entity) and entity[-1][1].startswith(\"IN\"):\n",
    "            entity.pop()\n",
    "        if(entity and \" \".join(e[0] for e in entity)[0].isupper()):\n",
    "            custom_ner.append(\" \".join(e[0] for e in entity))\n",
    "        entity = []\n",
    "print(custom_ner)\n",
    "count = Counter(custom_ner)\n",
    "sort_custom_ner = sorted(count.items(), key=lambda count:count[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom entity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def wiki(name):\n",
    "    try:\n",
    "        page = wikipedia.page(name)\n",
    "        summary = page.summary\n",
    "    except:\n",
    "        return \"\"\n",
    "    return nltk.sent_tokenize(summary)[0]\n",
    "\n",
    "def wikidescription(name):\n",
    "    sent = wiki(name)\n",
    "    if sent == \"\":\n",
    "        return \"a Thing\"\n",
    "    \n",
    "    text_pos = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    \n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN|NNS>}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(text_pos)\n",
    "    data = {}\n",
    "    for entity in result:\n",
    "        if isinstance(entity, nltk.tree.Tree):\n",
    "            text = \" \".join([word for word, tag in entity.leaves()])\n",
    "            ent = entity.label()\n",
    "            data[text] = ent\n",
    "        else:\n",
    "            continue\n",
    "    str = \"\"\n",
    "    for data in data:\n",
    "        str+=data\n",
    "        if (str[-1] != ' '):\n",
    "            str += ' '\n",
    "    return str\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikipedia-based classification using nltk entities as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yekaterinburg  -  Екатеринбу́рг [ jɪkətʲɪrʲɪnˈburk the fourth-largest city the administrative centre east the middle the Eurasian continent the Asian side the boundary \n",
      "Syria  -  الجمهورية السورية‎ al-Jumhūrīyah al-ʻArabīyah as-Sūrīyah a country the southwest the west the north the east the south \n",
      "Federation Council  -  Сове́т Федера́ции common abbreviation Совфед the upper house the parliament \n",
      "State Duma  -  Госуда́рственная ду́ма tr \n",
      "Address  -  An address a collection information format the location a building apartment other structure a plot land political boundaries street names references other identifiers house numbers \n",
      "GDP  -  domestic product a monetary measure the market value the final goods services a period time capita reflect differences the cost living the inflation rates the countries a basis power parity differences standards nations \n",
      "Particular  -  metaphysics particulars concrete spatiotemporal entities entities properties numbers \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\majkl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html5lib\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\users\\majkl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html5lib\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical  -  a Thing\n",
      "Crimean Bridge  -  Крымский мост tr \n",
      "Azov  -  Азов the stress the second syllable a town kilometers mi name the town \n",
      "Arctic  -  a polar region part \n",
      "Spatial Development Strategy  -  a document \n",
      "Extreme North  -  Крайний Север a large part enormous mineral natural resources \n",
      "Popular  -  a Thing\n",
      "Medical  -  the science practice the diagnosis prognosis treatment prevention disease \n",
      "Volunteers  -  a Thing\n",
      "NPOs  -  a Thing\n",
      "Sciences  -  Science word scientia knowledge a systematic enterprise the form testable explanations predictions roots science \n",
      "Council  -  A council a group people decisions \n",
      "Science  -  Science word scientia knowledge a systematic enterprise the form testable explanations predictions roots science \n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in sort_ner:\n",
    "    print(i[0], ' - ', wikidescription(i[0]))\n",
    "    c+= 1\n",
    "    if c == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wikipedia-based classification using custom patterns as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citizens  -  Citizenship the status a person the custom law a legal member a sovereign state belonging a nation \n",
      "members  -  a Thing\n",
      "special landmark  -  a private college disabilities attention disorders autism \n",
      "event  -  a Thing\n",
      "the times  -  a British daily national newspaper \n",
      "the choices  -  decision making \n",
      "every step  -  Every Little Step American singer Edmonds \n",
      "the future  -  The future the time \n",
      "country  -  A country a region a distinct entity political geography \n",
      "decades  -  A decade a period years \n",
      "points  -  a Thing\n",
      "time  -  the indefinite continued progress existence events irreversible succession the past the future \n",
      "ability  -  a Thing\n",
      "new territories  -  新界 Cantonese Yale Sān'gaai main regions \n",
      "build cities  -  SimCity a city-building simulation mobile game \n",
      "conquer  -  a Thing\n",
      "space  -  Space the boundless three-dimensional extent objects events relative position direction \n",
      "major discoveries  -  The timeline the date publication possible major scientific theories discoveries the discoverer \n",
      "This unwavering forward-looking drive  -  The Indian general election members parliament parliamentary constituencies \n",
      "traditions  -  A tradition a belief behavior a group society symbolic meaning special significance origins the past \n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in sort_custom_ner:\n",
    "    print(i[0], ' - ', wikidescription(i[0]))\n",
    "    c+= 1\n",
    "    if c == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
